{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Prompts, Chains, and Message History\n",
    "\n",
    "This notebook covers:\n",
    "- **Prompt Templates** - SystemMessage, HumanMessage, ChatPrompt with variables\n",
    "- **Output Parsers** - StrOutputParser for clean text output\n",
    "- **Chains** - Combining templates, models, and parsers\n",
    "- **Parallel Chains** - RunnableParallel for concurrent execution\n",
    "- **Chain Routing** - Conditional branching based on input\n",
    "- **Lambda Chains** - Inline transformations\n",
    "- **Passthrough** - Preserving original data through chains\n",
    "- **Message History** - Maintaining conversation context with SQLChatMessageHistory\n",
    "- **Custom Functions** - Building reusable chat functions\n",
    "\n",
    "Works with both **Anthropic Claude** and **Ollama** models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic langchain-ollama langchain-core langchain-community python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    "    chain\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Toggle between Anthropic Claude and Ollama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection\n",
    "USE_OLLAMA = False  # Set to True for Ollama, False for Anthropic\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    model = ChatOllama(\n",
    "        model=\"llama3.2\",\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(\"ðŸ¦™ Using Ollama (llama3.2)\")\n",
    "else:\n",
    "    model = ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "        api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    )\n",
    "    print(\"ðŸ¤– Using Anthropic Claude (claude-3-5-sonnet-20241022)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Prompt Templates\n",
    "\n",
    "## What are Prompt Templates?\n",
    "\n",
    "Prompt templates allow you to create reusable prompts with variables. Instead of hardcoding prompts, you can create templates with placeholders that get filled in at runtime.\n",
    "\n",
    "### Benefits:\n",
    "- **Reusability** - Write once, use many times\n",
    "- **Consistency** - Same prompt structure across use cases\n",
    "- **Maintainability** - Change prompts in one place\n",
    "- **Type Safety** - Define expected variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: SystemMessage and HumanMessage Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system message template with a role\n",
    "system = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a {role}. Your job is to help and guide junior developers.\"\n",
    ")\n",
    "\n",
    "# Create a human message template with topic variable\n",
    "question = HumanMessagePromptTemplate.from_template(\n",
    "    \"Explain {topic} with its real-world application and trade-offs if any.\"\n",
    ")\n",
    "\n",
    "# Display the templates\n",
    "print(\"System Template:\", system)\n",
    "print(\"\\nQuestion Template:\", question)\n",
    "\n",
    "# Format with actual values\n",
    "formatted_question = question.format(topic='RDD')\n",
    "print(\"\\nFormatted Question:\", formatted_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: ChatPromptTemplate\n",
    "\n",
    "Combine multiple message templates into a single chat prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt from the messages\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "print(\"Chat Template:\", template)\n",
    "print(\"\\nInput Variables:\", template.input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using the Template with a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the template with variables to create a full prompt\n",
    "question_prompt = template.invoke({\n",
    "    'role': 'AWS Cloud Architect',\n",
    "    'topic': 'Amazon RedShift'\n",
    "})\n",
    "\n",
    "# Send to the model\n",
    "response = model.invoke(question_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Chains and Output Parsers\n",
    "\n",
    "## What are Chains?\n",
    "\n",
    "Chains connect multiple components together:\n",
    "- **Prompt Template** â†’ **Model** â†’ **Output Parser**\n",
    "\n",
    "Using the `|` operator (pipe), we can chain components elegantly.\n",
    "\n",
    "## StrOutputParser\n",
    "\n",
    "Extracts just the text content from model responses, removing metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Basic Chain with StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain: template | model | parser\n",
    "chains = template | model\n",
    "\n",
    "# Invoke the chain\n",
    "result = chains.invoke({'role': 'Data Engineer', 'topic': 'Data Lakes'})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Chain with StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output parser to get clean string output\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = template | model | StrOutputParser()\n",
    "\n",
    "# Now response is just a string\n",
    "response = chain.invoke({'role': 'DevOps Engineer', 'topic': 'Infrastructure as Code'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Composed Chains and Summarization\n",
    "\n",
    "Chain outputs can feed into other chains, enabling complex workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summarization prompt\n",
    "summarization_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Your job is to analyze the text provided to you and provide\n",
    "    a concise summary not more than 100 words.\n",
    "    Text = {response}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create summarization chain\n",
    "summarization_chain = summarization_prompt | model | StrOutputParser()\n",
    "\n",
    "# Use the output from previous chain\n",
    "output = summarization_chain.invoke(response)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Composed Chain Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both chains into one\n",
    "# First get detailed response, then summarize it\n",
    "composed_chain = {\"response\": chain} | summarization_chain\n",
    "\n",
    "final_output = composed_chain.invoke({\n",
    "    'role': 'Software Engineer',\n",
    "    'topic': 'Generative AI using Bedrock'\n",
    "})\n",
    "\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Guidance and Leadership Chains\n",
    "\n",
    "Create specialized chains for different perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Multiple Perspective Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question template for best practices\n",
    "q1 = HumanMessagePromptTemplate.from_template(\n",
    "    'What are the best practices for implementing the {topic} in {technology}?'\n",
    ")\n",
    "\n",
    "# Build guidance chain\n",
    "msg1 = [system, q1]\n",
    "chat_template1 = ChatPromptTemplate.from_messages(msg1)\n",
    "guidance_chain = chat_template1 | model | StrOutputParser()\n",
    "\n",
    "# Test guidance chain\n",
    "output1 = guidance_chain.invoke({\n",
    "    'role': 'Principal Architect',\n",
    "    'topic': 'Designing APIs for Banking System',\n",
    "    'team': 'Backend Development team',\n",
    "    'technology': 'Python'\n",
    "})\n",
    "\n",
    "print(\"\\n=== GUIDANCE RESPONSE ===\")\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9: Leadership Perspective Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question template for leadership best practices\n",
    "q2 = HumanMessagePromptTemplate.from_template(\n",
    "    'As a leader, what are some best practices you would recommend for {team} when working on {topic} using {technology}?'\n",
    ")\n",
    "\n",
    "# Build leadership chain\n",
    "msg2 = [system, q2]\n",
    "chat_template2 = ChatPromptTemplate.from_messages(msg2)\n",
    "leadership_chain = chat_template2 | model | StrOutputParser()\n",
    "\n",
    "# Test leadership chain\n",
    "output2 = leadership_chain.invoke({\n",
    "    'role': 'Principal Architect',\n",
    "    'topic': 'Designing APIs for Banking System',\n",
    "    'team': 'Backend Development team',\n",
    "    'technology': 'Python'\n",
    "})\n",
    "\n",
    "print(\"\\n=== LEADERSHIP RESPONSE ===\")\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: RunnableParallel - Concurrent Execution\n",
    "\n",
    "Run multiple chains in parallel and get all results at once.\n",
    "\n",
    "**Benefits:**\n",
    "- Faster execution (parallel vs sequential)\n",
    "- Get multiple perspectives simultaneously\n",
    "- Structured output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10: Parallel Chain Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both chains in parallel\n",
    "chain_parallel = RunnableParallel(\n",
    "    guidance=guidance_chain,\n",
    "    leadership=leadership_chain\n",
    ")\n",
    "\n",
    "# Execute both chains at once\n",
    "parallel_result = chain_parallel.invoke({\n",
    "    'role': 'Principal Architect',\n",
    "    'topic': 'Designing APIs for Banking System',\n",
    "    'team': 'Backend Development team',\n",
    "    'technology': 'Python'\n",
    "})\n",
    "\n",
    "print(\"\\n=== PARALLEL EXECUTION RESULTS ===\")\n",
    "print(\"\\nðŸ“‹ Response from Guidance Chain:\")\n",
    "print(parallel_result['guidance'])\n",
    "print(\"\\nðŸ‘” Response from Leadership Chain:\")\n",
    "print(parallel_result['leadership'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 11: Using @chain Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom chain using decorator\n",
    "@chain\n",
    "def custom_chain(params):\n",
    "    return {\n",
    "        'guidance': guidance_chain.invoke(params),\n",
    "        'leadership': leadership_chain.invoke(params)\n",
    "    }\n",
    "\n",
    "# Test custom chain\n",
    "params = {\n",
    "    'role': 'Principal Architect',\n",
    "    'topic': 'Designing APIs for Banking System',\n",
    "    'team': 'Backend Development team',\n",
    "    'technology': 'Python'\n",
    "}\n",
    "\n",
    "output = custom_chain.invoke(params)\n",
    "print('Response from Guidance Chain:')\n",
    "print(output['guidance'])\n",
    "print('\\n Response from Leadership Chain:')\n",
    "print(output['leadership'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Chain Router - Conditional Branching\n",
    "\n",
    "Route to different chains based on input conditions.\n",
    "\n",
    "**Use Cases:**\n",
    "- Sentiment analysis â†’ different response templates\n",
    "- Classification â†’ different processing pipelines\n",
    "- Priority routing â†’ different handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 12: Sentiment-Based Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment classifier prompt\n",
    "prompt = \"\"\"\n",
    "Given the user review below, classify it as either being about 'Positive' or 'Negative'.\n",
    "Do not respond with more than one word.\n",
    "\n",
    "Review: {review}\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "chain = template | model | StrOutputParser()\n",
    "\n",
    "# Test sentiment classification\n",
    "review = \"The product quality is excellent and the customer service was very helpful.\"\n",
    "sentiment = chain.invoke({'review': review})\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 13: Creating Response Chains for Each Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive review response template\n",
    "positive_prompt = \"\"\"\n",
    "You are expert in writing reply for positive reviews.\n",
    "You need to encourage the customer to share their experience on social media\n",
    "and ask them to recommend the product to their friends and family.\n",
    "Review: {review}\n",
    "Reply:\n",
    "\"\"\"\n",
    "\n",
    "positive_template = ChatPromptTemplate.from_template(positive_prompt)\n",
    "positive_chain = positive_template | model | StrOutputParser()\n",
    "\n",
    "# Negative review response template\n",
    "negative_prompt = \"\"\"\n",
    "You are expert in writing reply for negative reviews.\n",
    "You need to apologize for the inconvenience caused and offer a solution to resolve the issue.\n",
    "You need to encourage the customer to share their concern on the following\n",
    "email address: 'support@example.com'.\n",
    "Review: {review}\n",
    "Reply:\n",
    "\"\"\"\n",
    "\n",
    "negative_template = ChatPromptTemplate.from_template(negative_prompt)\n",
    "negative_chain = negative_template | model | StrOutputParser()\n",
    "\n",
    "print(\"âœ… Response chains created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 14: Route Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define routing function\n",
    "def route(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return positive_chain\n",
    "    else:\n",
    "        return negative_chain\n",
    "\n",
    "# Test routing\n",
    "test_info = {'sentiment': 'Positive', 'review': review}\n",
    "selected_chain = route(test_info)\n",
    "print(f\"Selected chain: {selected_chain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 15: Complete Routing Chain with RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Build complete routing chain\n",
    "full_chain = {\n",
    "    'sentiment': chain,\n",
    "    'review': lambda x: x['review']\n",
    "} | RunnableLambda(route) | StrOutputParser()\n",
    "\n",
    "# Test with positive review\n",
    "review1 = \"The product quality is excellent and the customer service was very helpful.\"\n",
    "response1 = full_chain.invoke({'review': review1})\n",
    "print(\"\\n=== POSITIVE REVIEW RESPONSE ===\")\n",
    "print(response1)\n",
    "\n",
    "# Test with negative review\n",
    "review2 = \"The product stopped working after a week and the customer service was unresponsive.\"\n",
    "response2 = full_chain.invoke({'review': review2})\n",
    "print(\"\\n=== NEGATIVE REVIEW RESPONSE ===\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: RunnablePassthrough\n",
    "\n",
    "Pass data through a chain without modification, useful for preserving context.\n",
    "\n",
    "**Use Cases:**\n",
    "- Keep original input alongside transformations\n",
    "- Add metadata to chain outputs\n",
    "- Debugging and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 16: Helper Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "# Test functions\n",
    "test_text = \"Hello world, this is a test\"\n",
    "print(f\"Words: {count_words(test_text)}\")\n",
    "print(f\"Characters: {count_chars(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 17: Chain with Passthrough and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain that includes analysis\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain the concept of {topic} in {technology}. Keep the explanation concise and max 3 lines.\"\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser() | {\n",
    "    'word_count': lambda x: count_words(x),\n",
    "    'char_count': lambda x: count_chars(x),\n",
    "    'output': RunnablePassthrough()\n",
    "}\n",
    "\n",
    "# Execute chain\n",
    "output = chain.invoke({'topic': 'Async', 'technology': 'Python'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Message History and Conversation Memory\n",
    "\n",
    "Maintain conversation context across multiple interactions.\n",
    "\n",
    "**Components:**\n",
    "- `SQLChatMessageHistory` - Store messages in SQLite database\n",
    "- `RunnableWithMessageHistory` - Automatically manage conversation history\n",
    "- `MessagesPlaceholder` - Template variable for message history\n",
    "\n",
    "**Benefits:**\n",
    "- Context-aware responses\n",
    "- Multi-turn conversations\n",
    "- Persistent memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 18: Basic Conversation (No Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chat chain\n",
    "template = ChatPromptTemplate.from_template(\"{prompt}\")\n",
    "chain = template | model | StrOutputParser()\n",
    "\n",
    "# First query\n",
    "query = \"I am currently working as a Generative AI Engineer at Vanguard.\"\n",
    "response1 = chain.invoke({'prompt': query})\n",
    "print(\"Response 1:\", response1)\n",
    "\n",
    "# Second query (no memory - model won't remember previous message)\n",
    "response2 = chain.invoke({'prompt': \"What is my current job title?\"})\n",
    "print(\"\\nResponse 2:\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 19: Setting Up Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get or create message history for a session\n",
    "def get_session_history(session_id: str) -> SQLChatMessageHistory:\n",
    "    \"\"\"Fetch message history for a given session ID from SQL database.\"\"\"\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///chat_history.db\")\n",
    "\n",
    "# Create template with history placeholder\n",
    "template = ChatPromptTemplate.from_template(\"{prompt}\")\n",
    "chain = template | model | StrOutputParser()\n",
    "\n",
    "# Wrap chain with message history\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history\n",
    ")\n",
    "\n",
    "print(\"âœ… Message history configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 20: Conversation with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User session ID\n",
    "user_id = 'aniket_0123'\n",
    "\n",
    "# Get history for this user\n",
    "history = get_session_history(user_id)\n",
    "\n",
    "# First message\n",
    "runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"I am currently working as a Generative AI Engineer at Vanguard.\")],\n",
    "    config={\"configurable\": {\"session_id\": user_id}}\n",
    ")\n",
    "\n",
    "# View message history\n",
    "print(\"\\n=== Message History ===\")\n",
    "messages = history.get_messages()\n",
    "for msg in messages:\n",
    "    print(f\"{type(msg).__name__}: {msg.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 21: Follow-up Questions with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask follow-up question (model now has context)\n",
    "response = runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"What is my current job title?\")],\n",
    "    config={\"configurable\": {\"session_id\": user_id}}\n",
    ")\n",
    "\n",
    "print(\"\\n=== Response with Context ===\")\n",
    "print(response)\n",
    "\n",
    "# Check updated history\n",
    "print(\"\\n=== Updated Message History ===\")\n",
    "messages = history.get_messages()\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"{i}. {type(msg).__name__}: {msg.content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 22: Advanced - History with System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create template with system message and history\n",
    "system = SystemMessagePromptTemplate.from_template(\n",
    "    template=\"You are a helpful assistant.\"\n",
    ")\n",
    "human = HumanMessagePromptTemplate.from_template(\n",
    "    template=\"{input}\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    system,\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    human\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='input',\n",
    "    history_messages_key='history'\n",
    ")\n",
    "\n",
    "print(\"âœ… Advanced chain with history configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 23: Custom Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llm(user_input: str, session_id: str) -> str:\n",
    "    \"\"\"Chat with LLM while maintaining message history.\"\"\"\n",
    "    return runnable_with_history.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "# Test the chat function\n",
    "user_id = \"user-1254\"\n",
    "\n",
    "response1 = chat_with_llm(\"My name is Gpt Smith, and I make AI models.\", user_id)\n",
    "print(\"Response 1:\", response1)\n",
    "\n",
    "response2 = chat_with_llm(\"What is my name?\", user_id)\n",
    "print(\"\\nResponse 2:\", response2)\n",
    "\n",
    "response3 = chat_with_llm(\"What do I do for work?\", user_id)\n",
    "print(\"\\nResponse 3:\", response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Key Concepts\n",
    "\n",
    "## 1. Prompt Templates\n",
    "- **SystemMessagePromptTemplate** - Define AI role/behavior\n",
    "- **HumanMessagePromptTemplate** - User message with variables\n",
    "- **ChatPromptTemplate** - Combine multiple messages\n",
    "\n",
    "## 2. Chains\n",
    "- **Basic Chain** - `template | model | parser`\n",
    "- **Composed Chains** - Output of one chain feeds into another\n",
    "- **Parallel Chains** - `RunnableParallel` for concurrent execution\n",
    "\n",
    "## 3. Routing\n",
    "- **Conditional Logic** - Route to different chains based on input\n",
    "- **RunnableLambda** - Custom transformation functions\n",
    "\n",
    "## 4. Data Flow\n",
    "- **RunnablePassthrough** - Preserve original data\n",
    "- **Lambda Functions** - Inline data transformations\n",
    "\n",
    "## 5. Message History\n",
    "- **SQLChatMessageHistory** - Persistent conversation storage\n",
    "- **RunnableWithMessageHistory** - Automatic history management\n",
    "- **MessagesPlaceholder** - Template variable for history\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Use Templates** - Makes prompts reusable and maintainable\n",
    "2. **Chain Composition** - Build complex workflows from simple components\n",
    "3. **Output Parsing** - Use `StrOutputParser()` for clean text output\n",
    "4. **Parallel Execution** - Use `RunnableParallel` for multiple perspectives\n",
    "5. **Routing** - Branch logic based on classification/sentiment\n",
    "6. **Message History** - Essential for multi-turn conversations\n",
    "7. **Session Management** - Use unique session IDs for different users\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "- **Error Handling** - Wrap chains in try-except blocks\n",
    "- **Rate Limiting** - Respect API limits\n",
    "- **Cost Management** - Monitor token usage\n",
    "- **Database** - Use PostgreSQL for production (not SQLite)\n",
    "- **Logging** - Track conversations for debugging\n",
    "- **Security** - Sanitize user inputs\n",
    "- **Privacy** - Handle sensitive data appropriately"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
