{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Tutorial with Claude\n",
    "\n",
    "This notebook demonstrates various LangChain features including:\n",
    "- Basic chat interactions\n",
    "- Batch processing\n",
    "- Streaming responses\n",
    "- Tool usage (function calling)\n",
    "- System messages and prompts\n",
    "- Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, install required packages:\n",
    "```bash\n",
    "pip install langchain-anthropic langchain-ollama python-dotenv langchain langchain-core\n",
    "```\n",
    "\n",
    "For Ollama, make sure you have Ollama installed and running locally:\n",
    "```bash\n",
    "# Install Ollama from https://ollama.ai\n",
    "# Then pull a model, for example:\n",
    "ollama pull llama3.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model (Anthropic or Ollama)\n",
    "\n",
    "Choose which model to use by setting the `USE_OLLAMA` variable.\n",
    "- Set to `False` for Anthropic Claude (requires ANTHROPIC_API_KEY)\n",
    "- Set to `True` for Ollama (requires Ollama running locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model...\n",
      "Model initialized: Ollama (llama3.1)\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Choose which model to use\n",
    "USE_OLLAMA = True  # Set to True to use Ollama, False to use Anthropic\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    # Initialize Ollama model\n",
    "    print(\"Using Ollama model...\")\n",
    "    model = ChatOllama(\n",
    "        model=\"llama3.2\",  # or \"llama2\", \"mistral\", \"codellama\", etc.\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(f\"Model initialized: Ollama (llama3.1)\")\n",
    "else:\n",
    "    # Initialize Anthropic Claude model\n",
    "    print(\"Using Anthropic Claude model...\")\n",
    "    model = ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20241022\",  # or \"claude-3-opus-20240229\", \"claude-3-haiku-20240307\"\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "        api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    )\n",
    "    print(f\"Model initialized: Claude (claude-3-5-sonnet-20241022)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Switch Between Models\n",
    "\n",
    "Use this function to easily switch between Anthropic and Ollama models during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(use_ollama=False, ollama_model=\"llama3.1\", anthropic_model=\"claude-3-5-sonnet-20241022\"):\n",
    "    \"\"\"\n",
    "    Get a chat model instance.\n",
    "    \n",
    "    Args:\n",
    "        use_ollama (bool): If True, use Ollama; if False, use Anthropic\n",
    "        ollama_model (str): Ollama model name (e.g., 'llama3.1', 'mistral', 'codellama')\n",
    "        anthropic_model (str): Anthropic model name (e.g., 'claude-3-5-sonnet-20241022')\n",
    "    \n",
    "    Returns:\n",
    "        ChatModel: Initialized chat model instance\n",
    "    \"\"\"\n",
    "    if use_ollama:\n",
    "        print(f\"Initializing Ollama model: {ollama_model}\")\n",
    "        return ChatOllama(\n",
    "            model=ollama_model,\n",
    "            base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Initializing Anthropic model: {anthropic_model}\")\n",
    "        return ChatAnthropic(\n",
    "            model=anthropic_model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=4000,\n",
    "            api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        )\n",
    "\n",
    "# Example usage:\n",
    "# model = get_model(use_ollama=False)  # Use Anthropic\n",
    "# model = get_model(use_ollama=True, ollama_model=\"llama3.1\")  # Use Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "### Anthropic Models:\n",
    "- `claude-3-5-sonnet-20241022` - Most capable, balanced performance\n",
    "- `claude-3-opus-20240229` - Most powerful for complex tasks\n",
    "- `claude-3-haiku-20240307` - Fastest, most affordable\n",
    "\n",
    "### Popular Ollama Models:\n",
    "- `llama3.1` - Meta's latest Llama model\n",
    "- `llama2` - Previous generation Llama\n",
    "- `mistral` - Mistral AI's model\n",
    "- `codellama` - Specialized for coding\n",
    "- `phi3` - Microsoft's efficient model\n",
    "- `gemma` - Google's open model\n",
    "\n",
    "Run `ollama list` in your terminal to see installed models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Basic Chat Interaction\n",
    "\n",
    "Simple question-answer interaction with Claude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "# Ask a simple question\n",
    "response = model.invoke(\"What is the capital of India?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Streaming Response\n",
    "\n",
    "Stream the response token by token for a more interactive experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's something interesting about India:\n",
      "\n",
      "Did you know that India is home to the world's largest film industry, producing over 1,000 films a year? The Indian film industry, also known as Bollywood, produces more movies than Hollywood. In fact, it's estimated that over 45% of the world's films are produced in India! This is not surprising, given India's rich cultural heritage and its vibrant music and dance traditions.\n",
      "\n",
      "India is also home to some incredible ancient ruins and monuments. The city of Hampi in southern India is a UNESCO World Heritage Site and features an impressive array of temples, palaces, and other structures built by the Vijayanagara Empire over 500 years ago. Another notable site is the Khajuraho temple complex in central India, which is famous for its intricate carvings depicting erotic scenes from Hindu mythology.\n",
      "\n",
      "India has a staggering diversity of languages and cultures. With 22 officially recognized languages and hundreds of dialects spoken across the country, it's estimated that over 1,600 languages are spoken in India! This linguistic diversity is reflected in the country's cuisine, music, dance, art, and literature as well. From the spicy curries of southern India to the rich textiles of Kashmir, each region has its own unique flavor and flair."
     ]
    }
   ],
   "source": [
    "# Stream a response about India\n",
    "for chunk in model.stream(\"Tell me something interesting about India in three paragraphs\"):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Batch Processing\n",
    "\n",
    "Process multiple prompts efficiently using batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist!', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2026-01-30T04:09:05.186193Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5938489250, 'load_duration': 939872459, 'prompt_eval_count': 32, 'prompt_eval_duration': 4175000000, 'eval_count': 21, 'eval_duration': 820000000, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--019c0d17-0842-7ba2-ba83-b42a862696d8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 32, 'output_tokens': 21, 'total_tokens': 53}),\n",
       " AIMessage(content='The Theory of Relativity is a fundamental concept in physics that was developed by Albert Einstein. It\\'s actually made up of two main parts: Special Relativity and General Relativity.\\n\\n**Special Relativity (1905)**\\n\\nImagine you\\'re on a train, and you throw a ball straight up in the air. What happens? The ball comes down and lands in your hand, right?\\n\\nNow, imagine someone is standing outside the train, watching you as you move by. From their perspective, the ball doesn\\'t just go straight up – it also moves horizontally with the train.\\n\\nHere\\'s the key point: time and space are connected. When you\\'re on the train, time passes normally for you. But when you\\'re standing outside, time seems to pass a bit slower for you because you\\'re moving really fast compared to someone else.\\n\\nThis effect is called \"time dilation.\" It means that time can appear to slow down or speed up depending on how fast you\\'re moving and where you are in the universe.\\n\\n**General Relativity (1915)**\\n\\nImagine you have a heavy ball sitting on a trampoline. The trampoline will warp and curve, right?\\n\\nNow, imagine space is like that trampoline. According to General Relativity, massive objects like planets and stars warp and curve space around them, creating gravity.\\n\\nThink of it like this: if you roll a marble near a heavy object, it will follow the curved path of the space around it, which we experience as gravity.\\n\\n**Key Takeaways**\\n\\n1. **Time is relative**: Time can appear to slow down or speed up depending on how fast you\\'re moving and where you are in the universe.\\n2. **Space is curved**: Massive objects warp and curve space around them, creating gravity.\\n3. **Gravity is not a force**: Instead, it\\'s a result of the curvature of space.\\n\\nThat\\'s the Theory of Relativity in a nutshell! It\\'s a mind-bending concept that changed our understanding of the universe and how we experience reality.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2026-01-30T04:09:16.123842Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16876070667, 'load_duration': 939152334, 'prompt_eval_count': 36, 'prompt_eval_duration': 4176000000, 'eval_count': 408, 'eval_duration': 11757000000, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--019c0d17-0842-7ba2-ba83-b41d509de7c3-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 36, 'output_tokens': 408, 'total_tokens': 444})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch process multiple queries\n",
    "model.batch(\n",
    "    [\"Tell me a joke about cats.\", \"Explain the theory of relativity in simple terms.\"],\n",
    "    config={\n",
    "        \"max_tokens\": 500,\n",
    "        \"max_concurrency\": 3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Tool Usage (Function Calling)\n",
    "\n",
    "Define tools that Claude can use to perform specific actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2026-01-30T04:10:19.386656Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4614909041, 'load_duration': 22165125, 'prompt_eval_count': 162, 'prompt_eval_duration': 4030000000, 'eval_count': 19, 'eval_duration': 561000000, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'} id='lc_run--019c0d18-2f6b-7613-b6f8-b7587cb6028d-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'New York City'}, 'id': '763f7aa4-9c7a-4390-bcd9-9554c73a1135', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 162, 'output_tokens': 19, 'total_tokens': 181}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# Define a simple weather tool\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather for a location.\"\"\"\n",
    "    # For demonstration purposes, we'll return a dummy weather report.\n",
    "    return f\"The weather in {location} is sunny with a high of 75°F.\"\n",
    "\n",
    "# Bind the tool to the model\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# Invoke the model with a weather query\n",
    "response = model_with_tools.invoke(\"What's the weather like in New York City?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Tool Usage with Manual Execution\n",
    "\n",
    "Demonstrate the complete tool calling flow:\n",
    "1. User asks a question\n",
    "2. Model requests a tool call\n",
    "3. Tool is executed\n",
    "4. Result is sent back to the model\n",
    "5. Model provides final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - AI Response:\n",
      "Content: \n",
      "Tool Calls: [{'name': 'get_temperature', 'args': {'city': 'London'}, 'id': '3370803a-7670-4a31-a6e8-0bebdf044fc1', 'type': 'tool_call'}]\n",
      "\n",
      "Step 2 - Tool Execution:\n",
      "Tool: get_temperature\n",
      "Args: {'city': 'London'}\n",
      "Result: 15°C\n",
      "\n",
      "Step 3 - Final AI Response:\n",
      "The current temperature in London is 15°C.\n",
      "\n",
      "============================================================\n",
      "Complete Message History:\n",
      "\n",
      "1. HumanMessage:\n",
      "   content=\"What's the temperature in London?\" additional_kwargs={} response_metadata={}\n",
      "\n",
      "2. AIMessage:\n",
      "   content='' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2026-01-30T04:11:54.896538Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5551419792, 'load_duration': 26273875, 'prompt_eval_count': 160, 'prompt_eval_duration': 4984000000, 'eval_count': 17, 'eval_duration': 538000000, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'} id='lc_run--019c0d19-a0d8-7e91-89ab-ef56f1697703-0' tool_calls=[{'name': 'get_temperature', 'args': {'city': 'London'}, 'id': '3370803a-7670-4a31-a6e8-0bebdf044fc1', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 160, 'output_tokens': 17, 'total_tokens': 177}\n",
      "\n",
      "3. ToolMessage:\n",
      "   content='15°C' tool_call_id='3370803a-7670-4a31-a6e8-0bebdf044fc1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a tool for temperature lookup\n",
    "@tool\n",
    "def get_temperature(city: str) -> str:\n",
    "    \"\"\"Get the current temperature for a city.\"\"\"\n",
    "    # Simulated weather data\n",
    "    temps = {\"London\": \"15°C\", \"Paris\": \"18°C\", \"Tokyo\": \"22°C\"}\n",
    "    return temps.get(city, \"Temperature data not available\")\n",
    "\n",
    "# Bind tool to model\n",
    "model_with_tools = model.bind_tools([get_temperature])\n",
    "\n",
    "# Step 1: User asks a question\n",
    "messages = [\n",
    "    HumanMessage(content=\"What's the temperature in London?\")\n",
    "]\n",
    "\n",
    "# Step 2: Model responds with a tool call\n",
    "response = model_with_tools.invoke(messages)\n",
    "print(\"Step 1 - AI Response:\")\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"Tool Calls: {response.tool_calls}\\n\")\n",
    "\n",
    "# Add AI response to messages\n",
    "messages.append(response)\n",
    "\n",
    "# Step 3: Execute the tool and create ToolMessage\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    \n",
    "    # Execute the tool\n",
    "    result = get_temperature.invoke(tool_call[\"args\"])\n",
    "    \n",
    "    # Create ToolMessage with the result\n",
    "    tool_message = ToolMessage(\n",
    "        content=result,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Step 2 - Tool Execution:\")\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")\n",
    "    print(f\"Result: {result}\\n\")\n",
    "    \n",
    "    # Add ToolMessage to conversation\n",
    "    messages.append(tool_message)\n",
    "    \n",
    "    # Step 4: Model generates final answer using tool result\n",
    "    final_response = model_with_tools.invoke(messages)\n",
    "    print(\"Step 3 - Final AI Response:\")\n",
    "    print(final_response.content)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Complete Message History:\")\n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        print(f\"\\n{i}. {type(msg).__name__}:\")\n",
    "        print(f\"   {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Using System Messages\n",
    "\n",
    "System messages help define the assistant's behavior and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import (SystemMessage,\n",
    "                                HumanMessage,\n",
    "                                AIMessage)\n",
    "\n",
    "# Create a conversation with system message defining expertise\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert travel guide.\"),\n",
    "    HumanMessage(content=\"I am planning a trip to Columbia. Can you suggest some must-visit places?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example with Python developer expertise\n",
    "system_msg = SystemMessage(\"\"\"\n",
    "You are senior Python Developer with 5 years of\n",
    "experience in building scalable ETL pipelines using PySpark.\n",
    "\"\"\")\n",
    "\n",
    "messages = [\n",
    "    system_msg,\n",
    "    HumanMessage(content=\"Explain how to optimize PySpark jobs for large datasets.\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Usage Metadata\n",
    "\n",
    "Check token usage and other metadata from the API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get usage metadata from a response\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Basic LangChain Agent\n",
    "\n",
    "Create an agent that can use tools autonomously to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Define a simple weather tool for the agent\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# Create an agent with the model and tools\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9: Compare Anthropic vs Ollama\n",
    "\n",
    "Compare responses from both models for the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"Explain what LangChain is in 2 sentences.\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANTHROPIC CLAUDE RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "anthropic_model = get_model(use_ollama=False)\n",
    "anthropic_response = anthropic_model.invoke(test_prompt)\n",
    "print(anthropic_response.content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OLLAMA RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "ollama_model = get_model(use_ollama=True, ollama_model=\"llama3.1\")\n",
    "ollama_response = ollama_model.invoke(test_prompt)\n",
    "print(ollama_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10: Model-Specific Features\n",
    "\n",
    "Some features may work differently between providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which model you're using\n",
    "def check_model_type(model):\n",
    "    if isinstance(model, ChatAnthropic):\n",
    "        print(\"Using Anthropic Claude\")\n",
    "        print(f\"Model: {model.model}\")\n",
    "        print(\"Features: Function calling, streaming, vision (some models)\")\n",
    "    elif isinstance(model, ChatOllama):\n",
    "        print(\"Using Ollama\")\n",
    "        print(f\"Model: {model.model}\")\n",
    "        print(f\"Base URL: {model.base_url}\")\n",
    "        print(\"Features: Streaming, local inference, no API costs\")\n",
    "    else:\n",
    "        print(\"Unknown model type\")\n",
    "\n",
    "check_model_type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Make sure to set your `ANTHROPIC_API_KEY` in the `.env` file\n",
    "- The code demonstrates various LangChain patterns with Claude\n",
    "- Tool usage allows Claude to interact with external functions\n",
    "- System messages help guide the model's behavior and expertise\n",
    "- Streaming is useful for real-time user experience\n",
    "- Batch processing helps optimize multiple queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
