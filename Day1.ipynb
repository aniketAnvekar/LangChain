{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Tutorial with Claude\n",
    "\n",
    "This notebook demonstrates various LangChain features including:\n",
    "- Basic chat interactions\n",
    "- Batch processing\n",
    "- Streaming responses\n",
    "- Tool usage (function calling)\n",
    "- System messages and prompts\n",
    "- Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, install required packages:\n",
    "```bash\n",
    "pip install langchain-anthropic langchain-ollama python-dotenv langchain langchain-core\n",
    "```\n",
    "\n",
    "For Ollama, make sure you have Ollama installed and running locally:\n",
    "```bash\n",
    "# Install Ollama from https://ollama.ai\n",
    "# Then pull a model, for example:\n",
    "ollama pull llama3.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model (Anthropic or Ollama)\n",
    "\n",
    "Choose which model to use by setting the `USE_OLLAMA` variable.\n",
    "- Set to `False` for Anthropic Claude (requires ANTHROPIC_API_KEY)\n",
    "- Set to `True` for Ollama (requires Ollama running locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Choose which model to use\n",
    "USE_OLLAMA = False  # Set to True to use Ollama, False to use Anthropic\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    # Initialize Ollama model\n",
    "    print(\"Using Ollama model...\")\n",
    "    model = ChatOllama(\n",
    "        model=\"llama3.1\",  # or \"llama2\", \"mistral\", \"codellama\", etc.\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(f\"Model initialized: Ollama (llama3.1)\")\n",
    "else:\n",
    "    # Initialize Anthropic Claude model\n",
    "    print(\"Using Anthropic Claude model...\")\n",
    "    model = ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20241022\",  # or \"claude-3-opus-20240229\", \"claude-3-haiku-20240307\"\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "        api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    )\n",
    "    print(f\"Model initialized: Claude (claude-3-5-sonnet-20241022)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Switch Between Models\n",
    "\n",
    "Use this function to easily switch between Anthropic and Ollama models during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(use_ollama=False, ollama_model=\"llama3.1\", anthropic_model=\"claude-3-5-sonnet-20241022\"):\n",
    "    \"\"\"\n",
    "    Get a chat model instance.\n",
    "    \n",
    "    Args:\n",
    "        use_ollama (bool): If True, use Ollama; if False, use Anthropic\n",
    "        ollama_model (str): Ollama model name (e.g., 'llama3.1', 'mistral', 'codellama')\n",
    "        anthropic_model (str): Anthropic model name (e.g., 'claude-3-5-sonnet-20241022')\n",
    "    \n",
    "    Returns:\n",
    "        ChatModel: Initialized chat model instance\n",
    "    \"\"\"\n",
    "    if use_ollama:\n",
    "        print(f\"Initializing Ollama model: {ollama_model}\")\n",
    "        return ChatOllama(\n",
    "            model=ollama_model,\n",
    "            base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Initializing Anthropic model: {anthropic_model}\")\n",
    "        return ChatAnthropic(\n",
    "            model=anthropic_model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=4000,\n",
    "            api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        )\n",
    "\n",
    "# Example usage:\n",
    "# model = get_model(use_ollama=False)  # Use Anthropic\n",
    "# model = get_model(use_ollama=True, ollama_model=\"llama3.1\")  # Use Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "### Anthropic Models:\n",
    "- `claude-3-5-sonnet-20241022` - Most capable, balanced performance\n",
    "- `claude-3-opus-20240229` - Most powerful for complex tasks\n",
    "- `claude-3-haiku-20240307` - Fastest, most affordable\n",
    "\n",
    "### Popular Ollama Models:\n",
    "- `llama3.1` - Meta's latest Llama model\n",
    "- `llama2` - Previous generation Llama\n",
    "- `mistral` - Mistral AI's model\n",
    "- `codellama` - Specialized for coding\n",
    "- `phi3` - Microsoft's efficient model\n",
    "- `gemma` - Google's open model\n",
    "\n",
    "Run `ollama list` in your terminal to see installed models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Chat Interaction\n",
    "\n",
    "Simple question-answer interaction with Claude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a simple question\n",
    "response = model.invoke(\"What is the capital of India?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Streaming Response\n",
    "\n",
    "Stream the response token by token for a more interactive experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream a response about India\n",
    "for chunk in model.stream(\"Tell me something interesting about India in three paragraphs\"):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Batch Processing\n",
    "\n",
    "Process multiple prompts efficiently using batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple queries\n",
    "model.batch(\n",
    "    [\"Tell me a joke about cats.\", \"Explain the theory of relativity in simple terms.\"],\n",
    "    config={\n",
    "        \"max_tokens\": 500,\n",
    "        \"max_concurrency\": 3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Tool Usage (Function Calling)\n",
    "\n",
    "Define tools that Claude can use to perform specific actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# Define a simple weather tool\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather for a location.\"\"\"\n",
    "    # For demonstration purposes, we'll return a dummy weather report.\n",
    "    return f\"The weather in {location} is sunny with a high of 75째F.\"\n",
    "\n",
    "# Bind the tool to the model\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# Invoke the model with a weather query\n",
    "response = model_with_tools.invoke(\"What's the weather like in New York City?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Tool Usage with Manual Execution\n",
    "\n",
    "Demonstrate the complete tool calling flow:\n",
    "1. User asks a question\n",
    "2. Model requests a tool call\n",
    "3. Tool is executed\n",
    "4. Result is sent back to the model\n",
    "5. Model provides final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a tool for temperature lookup\n",
    "@tool\n",
    "def get_temperature(city: str) -> str:\n",
    "    \"\"\"Get the current temperature for a city.\"\"\"\n",
    "    # Simulated weather data\n",
    "    temps = {\"London\": \"15째C\", \"Paris\": \"18째C\", \"Tokyo\": \"22째C\"}\n",
    "    return temps.get(city, \"Temperature data not available\")\n",
    "\n",
    "# Bind tool to model\n",
    "model_with_tools = model.bind_tools([get_temperature])\n",
    "\n",
    "# Step 1: User asks a question\n",
    "messages = [\n",
    "    HumanMessage(content=\"What's the temperature in London?\")\n",
    "]\n",
    "\n",
    "# Step 2: Model responds with a tool call\n",
    "response = model_with_tools.invoke(messages)\n",
    "print(\"Step 1 - AI Response:\")\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"Tool Calls: {response.tool_calls}\\n\")\n",
    "\n",
    "# Add AI response to messages\n",
    "messages.append(response)\n",
    "\n",
    "# Step 3: Execute the tool and create ToolMessage\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    \n",
    "    # Execute the tool\n",
    "    result = get_temperature.invoke(tool_call[\"args\"])\n",
    "    \n",
    "    # Create ToolMessage with the result\n",
    "    tool_message = ToolMessage(\n",
    "        content=result,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Step 2 - Tool Execution:\")\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")\n",
    "    print(f\"Result: {result}\\n\")\n",
    "    \n",
    "    # Add ToolMessage to conversation\n",
    "    messages.append(tool_message)\n",
    "    \n",
    "    # Step 4: Model generates final answer using tool result\n",
    "    final_response = model_with_tools.invoke(messages)\n",
    "    print(\"Step 3 - Final AI Response:\")\n",
    "    print(final_response.content)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Complete Message History:\")\n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        print(f\"\\n{i}. {type(msg).__name__}:\")\n",
    "        print(f\"   {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Using System Messages\n",
    "\n",
    "System messages help define the assistant's behavior and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import (SystemMessage,\n",
    "                                HumanMessage,\n",
    "                                AIMessage)\n",
    "\n",
    "# Create a conversation with system message defining expertise\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert travel guide.\"),\n",
    "    HumanMessage(content=\"I am planning a trip to Columbia. Can you suggest some must-visit places?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example with Python developer expertise\n",
    "system_msg = SystemMessage(\"\"\"\n",
    "You are senior Python Developer with 5 years of\n",
    "experience in building scalable ETL pipelines using PySpark.\n",
    "\"\"\")\n",
    "\n",
    "messages = [\n",
    "    system_msg,\n",
    "    HumanMessage(content=\"Explain how to optimize PySpark jobs for large datasets.\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Usage Metadata\n",
    "\n",
    "Check token usage and other metadata from the API response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get usage metadata from a response\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: LangChain Agents\n",
    "\n",
    "Create an agent that can use tools autonomously to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Define a simple weather tool for the agent\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# Create an agent with the model and tools\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9: Compare Anthropic vs Ollama\n",
    "\n",
    "Compare responses from both models for the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"Explain what LangChain is in 2 sentences.\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANTHROPIC CLAUDE RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "anthropic_model = get_model(use_ollama=False)\n",
    "anthropic_response = anthropic_model.invoke(test_prompt)\n",
    "print(anthropic_response.content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OLLAMA RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "ollama_model = get_model(use_ollama=True, ollama_model=\"llama3.1\")\n",
    "ollama_response = ollama_model.invoke(test_prompt)\n",
    "print(ollama_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10: Model-Specific Features\n",
    "\n",
    "Some features may work differently between providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which model you're using\n",
    "def check_model_type(model):\n",
    "    if isinstance(model, ChatAnthropic):\n",
    "        print(\"Using Anthropic Claude\")\n",
    "        print(f\"Model: {model.model}\")\n",
    "        print(\"Features: Function calling, streaming, vision (some models)\")\n",
    "    elif isinstance(model, ChatOllama):\n",
    "        print(\"Using Ollama\")\n",
    "        print(f\"Model: {model.model}\")\n",
    "        print(f\"Base URL: {model.base_url}\")\n",
    "        print(\"Features: Streaming, local inference, no API costs\")\n",
    "    else:\n",
    "        print(\"Unknown model type\")\n",
    "\n",
    "check_model_type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Make sure to set your `ANTHROPIC_API_KEY` in the `.env` file\n",
    "- The code demonstrates various LangChain patterns with Claude\n",
    "- Tool usage allows Claude to interact with external functions\n",
    "- System messages help guide the model's behavior and expertise\n",
    "- Streaming is useful for real-time user experience\n",
    "- Batch processing helps optimize multiple queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
